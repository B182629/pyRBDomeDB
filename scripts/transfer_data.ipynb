{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##This notebook finds the organism for each uniprot ID in the database and copies data from the SQLite database tables to mySQL tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "from Bio import Entrez, SeqIO\n",
    "from io import StringIO\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Adjust parameters in the settings.yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_file = open('/Users/aliceaakerberg/Sites/scripts/settings.yaml','r')\n",
    "settings = yaml.load(settings_file,Loader=yaml.FullLoader)\n",
    "\n",
    "db_path = settings[0]['database']['database_path']\n",
    "username = settings[0]['database']['username']\n",
    "password = settings[0]['database']['password']\n",
    "host = settings[0]['database']['host']\n",
    "database_name = settings[0]['database']['database_name']\n",
    "output_file_path = settings[1]['output']['output_file_path']\n",
    "Entrez.email = settings[2]['Entrez']['email']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 1: Retrieve organism data for each uniprot ID using NCBI Entrez. (This process can take a while (500 UniProt takes approx. 10min). Entrez may prematurely terminate execution if it deems the task excessive. It is recommended to split the UniProt IDs into small groups and execute script with these groups one by one to prevent premature termination.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'eutils.ncbi.nlm.nih.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organism: Homo sapiens\n"
     ]
    }
   ],
   "source": [
    "# This code chunk checks that Entrez E-utilities is working correctly. Ensure it does not produce an error before proceeding.\n",
    "\n",
    "url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "params = {\n",
    "    \"db\": \"protein\",\n",
    "    \"id\": \"B7ZW38\",\n",
    "    \"rettype\": \"gb\",\n",
    "    \"retmode\": \"text\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params, verify=False)\n",
    "response.raise_for_status()  # This will raise an HTTPError if the HTTP request returned an unsuccessful status code.\n",
    "\n",
    "handle = StringIO(response.text)\n",
    "record = SeqIO.read(handle, \"genbank\")\n",
    "handle.close()\n",
    "\n",
    "organism = record.annotations.get('organism', 'Organism not found')\n",
    "print(f'Organism: {organism}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_organism(db_path):\n",
    "    sqlite_conn = sqlite3.connect(db_path)\n",
    "    uniprot_df = pd.read_sql(f'SELECT ID FROM processed_files_log', sqlite_conn)\n",
    "    uniprot_ids = uniprot_df['ID'].tolist()\n",
    "    sqlite_conn.close()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for id in uniprot_ids:\n",
    "        params = {\n",
    "        \"db\": \"protein\",\n",
    "        \"id\": id,\n",
    "        \"rettype\": \"gb\",\n",
    "        \"retmode\": \"text\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params, verify=False)\n",
    "        response.raise_for_status()  # This will raise an HTTPError if the HTTP request returned an unsuccessful status code.\n",
    "\n",
    "        handle = StringIO(response.text)\n",
    "        record = SeqIO.read(handle, \"genbank\")\n",
    "        handle.close()\n",
    "\n",
    "        organism = record.annotations.get('organism', 'Organism not found')\n",
    "\n",
    "        results.append({\n",
    "            \"uniprot_id\": id,\n",
    "            \"organism\": organism\n",
    "        })\n",
    "\n",
    "        counter += 1\n",
    "        print(\"Organism fetched for UniProt \", id, \"(\", str(counter), \" out of \", str(len(uniprot_ids)), \")\")\n",
    "\n",
    "    # Step 3: Convert the list of results into a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organism_info = get_organism(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_protein_info(df_name, protein_info_mysql_table):    \n",
    "    conn = mysql.connector.connect(user=username, password=password, host=host, database=database_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"SELECT * FROM {protein_info_mysql_table}\")\n",
    "    existing_rows = cursor.fetchall()\n",
    "\n",
    "    # Create a set of tuples representing existing rows for fast lookup\n",
    "    existing_rows_set = set(existing_rows)\n",
    "\n",
    "    # Step 3: Insert DataFrame into the MySQL table without duplicates\n",
    "    columns = ', '.join(df_name.columns)\n",
    "    placeholders = ', '.join(['%s'] * len(df_name.columns))\n",
    "    insert_query = f\"INSERT INTO {protein_info_mysql_table} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "    for i, row in df_name.iterrows():\n",
    "        row_tuple = tuple(None if pd.isna(val) else val for val in row)\n",
    "        if row_tuple not in existing_rows_set:\n",
    "            try:\n",
    "                cursor.execute(insert_query, row_tuple)\n",
    "            except mysql.connector.Error as err:\n",
    "                print(f\"Error: {err}\")\n",
    "                print(f\"Failed to insert row: {row_tuple}\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    # Step 4: Close the cursor and connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Data imported successfully into table {protein_info_mysql_table}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported successfully into table protein_info!\n"
     ]
    }
   ],
   "source": [
    "protein_info_mysql_table = \"protein_info\"\n",
    "insert_protein_info(organism_info, protein_info_mysql_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 2: Retrieve SQlite table names from the database. These names will be used to iteratively copy data to the mySQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_names(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute a query to retrieve the table names\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Store the table names in a list\n",
    "    table_names = [table[0] for table in tables]\n",
    "    return table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = get_table_names(db_path)\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Step 3: Copy data from SQLite database to mySQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_sqlite_to_df(db_path, table_name, output_file_path):\n",
    "    # Connect to the SQLite3 database\n",
    "    sqlite_conn = sqlite3.connect(db_path)\n",
    "    # Read the data from the SQLite3 table into a pandas DataFrame\n",
    "    df = pd.read_sql(f'SELECT * FROM {table_name}', sqlite_conn)\n",
    "    sqlite_conn.close()\n",
    "\n",
    "    # Check for column names with whitespace and replace with underscores\n",
    "    original_columns = df.columns.tolist()\n",
    "    new_columns = [col.replace(' ', '_').replace('_-_', '_').replace('-', '_') for col in original_columns]\n",
    "\n",
    "    # Print which column names have been changed\n",
    "    for original, new in zip(original_columns, new_columns):\n",
    "        if original != new:\n",
    "            print(f\"Column name changed in '{table_name}': '{original}' to '{new}'\")\n",
    "\n",
    "    df.columns = new_columns\n",
    "\n",
    "    #Remove the # at the start of the next 4 lines to download database tables to output files directory\n",
    "\n",
    "    #os.makedirs(output_file_path, exist_ok=True)\n",
    "    #output_file_path = f\"{output_file_path}/{table_name}.tsv\"\n",
    "    #df.to_csv(output_file_path, sep='\\t', index=False)\n",
    "    #print(f\"Data from '{table_name}' saved to '{output_file_path}' successfully.\")\n",
    "\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    df.replace({np.inf: 'inf', -np.inf: '-inf'}, inplace=True)\n",
    "\n",
    "    conn = mysql.connector.connect(user=username, password=password, host=host, database=database_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "    existing_rows = cursor.fetchall()\n",
    "\n",
    "    # Create a set of tuples representing existing rows for fast lookup\n",
    "    existing_rows_set = set(existing_rows)\n",
    "\n",
    "    # Step 3: Insert DataFrame into the MySQL table without duplicates\n",
    "    columns = ', '.join(df.columns)\n",
    "    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "    insert_query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        row_tuple = tuple(None if pd.isna(val) else val for val in row)\n",
    "        if row_tuple not in existing_rows_set:\n",
    "            try:\n",
    "                cursor.execute(insert_query, row_tuple)\n",
    "            except mysql.connector.Error as err:\n",
    "                print(f\"Error: {err}\")\n",
    "                print(f\"Failed to insert row: {row_tuple}\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    # Step 4: Close the cursor and connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"Data imported successfully into table {table_name}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in table_names:\n",
    "    transfer_sqlite_to_df(db_path, table, output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrbdome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
